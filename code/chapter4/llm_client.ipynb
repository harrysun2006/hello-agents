{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1fa5239",
   "metadata": {},
   "source": [
    "### References\n",
    "- [Deploy and run LLM locally](../../notes/vLLM.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebe37ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM æ˜¯ä¸€ä¸ªå¼€æºçš„ï¼Œé«˜æ€§èƒ½çš„ LLM æ¨ç†æœåŠ¡æ¡†æ¶ï¼Œç”±å¡å¤«å¡ï¼ˆUC Berkeleyï¼‰å¼€å‘ã€‚å®ƒä¸»è¦è§£å†³ LLM éƒ¨ç½²ä¸­é‡åˆ°çš„é«˜ååé‡å’Œä½å»¶è¿Ÿé—®é¢˜ï¼Œå¹¶æä¾›äº†ä¸€ç³»åˆ—å…ˆè¿›çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œä½¿å¾— LLM å¯ä»¥åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­é«˜æ•ˆè¿è¡Œã€‚\n",
      "\n",
      "**ä»¥ä¸‹æ˜¯ vLLM çš„ä¸€äº›å…³é”®ç‰¹æ€§å’Œäº®ç‚¹ï¼š**\n",
      "\n",
      "* **PagedAttention:** è¿™æ˜¯ vLLM æœ€æ ¸å¿ƒçš„æŠ€æœ¯ã€‚ä¼ ç»Ÿ LLM æ¨ç†ä¼šä¸ºæ¯ä¸ªè¯·æ±‚çš„ token åˆ›å»ºä¸€ä¸ªæ³¨æ„åŠ›å—ï¼ˆAttention Blockï¼‰ï¼Œæ”¶é›†å®Œåæ‰é‡Šæ”¾ã€‚è¿™ä¼šå¯¼è‡´å¤§é‡å†…å­˜çš„æµªè´¹ï¼Œå°¤å…¶æ˜¯å½“æœ‰å¤šä¸ªè¯·æ±‚å¹¶å‘æ—¶ã€‚PagedAttention å€Ÿé‰´äº†æ“ä½œç³»ç»Ÿè™šæ‹Ÿå†…å­˜çš„ç®¡ç†æ€æƒ³ï¼Œå°†æ³¨æ„åŠ›å—åˆ’åˆ†ä¸ºæ›´å°çš„å—ï¼Œå¹¶åŠ¨æ€åœ°åˆ†é…å’Œé‡Šæ”¾å®ƒä»¬ï¼Œé¿å…äº†å†…å­˜ç¢ç‰‡åŒ–ï¼Œå¤§å¹…æé«˜äº†å†…å­˜åˆ©ç”¨ç‡å’Œæ¨ç†æ•ˆç‡ã€‚\n",
      "\n",
      "* **åŠ¨æ€æ‰¹å¤„ç† (Dynamic Batching):**  vLLM èƒ½å¤Ÿå°†å¤šä¸ªè¯·æ±‚åˆå¹¶æˆä¸€ä¸ªæ‰¹æ¬¡è¿›è¡Œå¤„ç†ï¼Œä»è€Œæœ€å¤§åŒ– GPU çš„åˆ©ç”¨ç‡ã€‚ä¸é™æ€æ‰¹å¤„ç†ç›¸æ¯”ï¼ŒåŠ¨æ€æ‰¹å¤„ç†å¯ä»¥æ›´çµæ´»åœ°è°ƒæ•´æ‰¹æ¬¡å¤§å°ï¼Œé€‚åº”ä¸åŒè¯·æ±‚çš„é•¿åº¦å’Œå¤æ‚åº¦ã€‚\n",
      "\n",
      "* **æ¨ç†ä¼˜åŒ–:** vLLM é‡‡ç”¨äº†å¤šç§æ¨ç†ä¼˜åŒ–æŠ€æœ¯ï¼Œä¾‹å¦‚é›¶å¼€é”€é‡åŒ– (Zero-Copy Quantization)ã€å¤š GPU æŒ‚è½½ (Multi-GPU Offloading) ç­‰ï¼Œè¿›ä¸€æ­¥æå‡äº†æ¨ç†é€Ÿåº¦ã€‚\n",
      "\n",
      "* **æ˜“ç”¨æ€§:** vLLM æä¾›äº†ä¸€ä¸ªç®€æ´å‹å¥½çš„ APIï¼Œæ–¹ä¾¿ç”¨æˆ·é›†æˆåˆ°ç°æœ‰çš„åº”ç”¨ä¸­ã€‚å®ƒæ”¯æŒå¤šç§ LLM æ¨¡å‹ï¼ŒåŒ…æ‹¬ Llama 2ã€Mistralã€Vicuna ç­‰ã€‚\n",
      "\n",
      "* **å¤šç§éƒ¨ç½²æ–¹å¼:** vLLM æ”¯æŒå¤šç§éƒ¨ç½²æ–¹å¼ï¼ŒåŒ…æ‹¬ï¼š\n",
      "    * **æœ¬åœ°éƒ¨ç½²:** å¯ä»¥åœ¨æœ¬åœ°æœºå™¨ä¸Šè¿è¡Œï¼Œé€‚ç”¨äºå®éªŒå’Œå¼€å‘ã€‚\n",
      "    * **äº‘ç«¯éƒ¨ç½²:** å¯ä»¥éƒ¨ç½²åˆ° Kubernetesã€Docker ç­‰å®¹å™¨åŒ–å¹³å°ä¸Šï¼Œé€‚ç”¨äºç”Ÿäº§ç¯å¢ƒã€‚\n",
      "\n",
      "**vLLM çš„ä¼˜åŠ¿ï¼š**\n",
      "\n",
      "* **æ›´é«˜çš„ååé‡:**  vLLM ç›¸æ¯”äºä¼ ç»Ÿçš„ llama.cpp ç­‰æ¨ç†å¼•æ“ï¼Œå¯ä»¥å®ç°æ›´é«˜çš„ååé‡ï¼Œå°¤å…¶æ˜¯åœ¨å¤š GPU ç¯å¢ƒä¸‹ã€‚\n",
      "* **æ›´ä½çš„å»¶è¿Ÿ:**  PagedAttention æ˜¾è‘—é™ä½äº†å»¶è¿Ÿï¼Œä½¿å¾— LLM çš„å“åº”é€Ÿåº¦æ›´å¿«ã€‚\n",
      "* **æ›´é«˜æ•ˆçš„å†…å­˜åˆ©ç”¨:**  PagedAttention é¿å…äº†å†…å­˜ç¢ç‰‡åŒ–ï¼Œå¤§å¹…æé«˜äº†å†…å­˜åˆ©ç”¨ç‡ã€‚\n",
      "* **æ˜“äºæ‰©å±•:**  vLLM çš„æ¶æ„è®¾è®¡ä¾¿äºæ‰©å±•ï¼Œå¯ä»¥æ”¯æŒæ›´å¤šæ¨¡å‹å’Œä¼˜åŒ–æŠ€æœ¯ã€‚\n",
      "\n",
      "**ä¸å…¶ä»– LLM æ¨ç†å¼•æ“çš„æ¯”è¾ƒ:**\n",
      "\n",
      "| ç‰¹æ€§        | vLLM         | llama.cpp   | Text Generation WebUI |\n",
      "|-------------|--------------|-------------|-----------------------|\n",
      "| æ ¸å¿ƒæŠ€æœ¯    | PagedAttention | åŸºäºä¾èµ–æŸ¥æ‰¾     | ç•Œé¢äº¤äº’ + ä¾èµ–æŸ¥æ‰¾    |\n",
      "| æ€§èƒ½        | ä¼˜ç§€         | è‰¯å¥½         | ä¸€èˆ¬                   |\n",
      "| å†…å­˜åˆ©ç”¨ç‡   | ä¼˜ç§€         | ä¸€èˆ¬         | ä¸€èˆ¬                   |\n",
      "| å¯æ‰©å±•æ€§     | ä¼˜ç§€         | ä¸€èˆ¬         | ä¸€èˆ¬                   |\n",
      "| éƒ¨ç½²æ–¹å¼      | å¤šç§         | æœ¬åœ°         | æœ¬åœ°                   |\n",
      "\n",
      "**æ€»è€Œè¨€ä¹‹ï¼ŒvLLM æ˜¯ä¸€æ¬¾é«˜æ•ˆã€æ˜“ç”¨ä¸”å¯æ‰©å±•çš„ LLM æ¨ç†æœåŠ¡æ¡†æ¶ï¼Œæ˜¯ç›®å‰ LLM éƒ¨ç½²é¢†åŸŸçš„ä¸€æ¬¾å¾ˆæœ‰å‰é€”çš„æŠ€æœ¯ã€‚**\n",
      "\n",
      "**è·å–æ›´å¤šä¿¡æ¯ï¼š**\n",
      "\n",
      "* **GitHub ä»“åº“:** [https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)\n",
      "* **å®˜æ–¹æ–‡æ¡£:** [https://vllm.ai/](https://vllm.ai/)\n",
      "\n",
      "å¸Œæœ›ä»¥ä¸Šä»‹ç»èƒ½å¤Ÿå¸®åŠ©ä½ æ›´å¥½åœ°äº†è§£ vLLMã€‚  ä½ è¿˜æœ‰ä»€ä¹ˆæƒ³çŸ¥é“çš„å—ï¼Ÿ æ¯”å¦‚ä½ æƒ³äº†è§£å®ƒçš„éƒ¨ç½²æ–¹æ³•ï¼Œæˆ–è€…å¦‚ä½•ä½¿ç”¨å®ƒè¿›è¡Œ LLM æ¨ç†å‘¢ï¼Ÿ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# hugging face TGI å…¼å®¹ OpenAI API\n",
    "client = OpenAI(base_url=\"http://localhost:8080/v1\", api_key=\"dummy\")\n",
    "# vLLM å…¼å®¹ OpenAI API\n",
    "# client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"dummy\")\n",
    "# Ollama å…¼å®¹ OpenAI API\n",
    "# client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"dummy\")\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    # model=\"Qwen3-0.6B\",\n",
    "    model=\"google/gemma-3-4b-it\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"ä»‹ç»ä¸€ä¸‹ vLLM æ˜¯ä»€ä¹ˆ\"}]\n",
    ")\n",
    "\n",
    "print(resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36b55680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  æ­£åœ¨è°ƒç”¨ Qwen/Qwen3-0.6B æ¨¡å‹ @ http://localhost:8080 ...\n",
      "âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: Error code: 404\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "\n",
    "# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡\n",
    "# load_dotenv()\n",
    "load_dotenv(dotenv_path='.vscode/.env.local', override=True)\n",
    "\n",
    "def main():\n",
    "    # model = os.getenv(\"LLM_MODEL_ID\")\n",
    "    # apiKey = os.getenv(\"LLM_API_KEY\")\n",
    "    # baseUrl = os.getenv(\"LLM_BASE_URL\")\n",
    "    # timeout = int(os.getenv(\"LLM_TIMEOUT\", 60))\n",
    "    model = \"Qwen/Qwen3-0.6B\"\n",
    "    apiKey = \"api_key\"\n",
    "    baseUrl = \"http://localhost:8080\"\n",
    "    timeout = 60\n",
    "    \n",
    "    try:\n",
    "        print(f\"ğŸ§  æ­£åœ¨è°ƒç”¨ {model} æ¨¡å‹ @ {baseUrl} ...\")\n",
    "        client = OpenAI(api_key=apiKey, base_url=baseUrl, timeout=timeout)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes Python code.\"},\n",
    "            {\"role\": \"user\", \"content\": \"å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•\"}\n",
    "        ]\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.1,\n",
    "            stream=True,\n",
    "        )\n",
    "        \n",
    "        # å¤„ç†æµå¼å“åº”\n",
    "        print(\"âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:\")\n",
    "        collected_content = []\n",
    "        for chunk in response:\n",
    "            content = chunk.choices[0].delta.content or \"\"\n",
    "            print(content, end=\"\", flush=True)\n",
    "            collected_content.append(content)\n",
    "        print()  # åœ¨æµå¼è¾“å‡ºç»“æŸåæ¢è¡Œ\n",
    "        responseText = \"\".join(collected_content)\n",
    "    \n",
    "        if responseText:\n",
    "            print(\"\\n\\n--- å®Œæ•´æ¨¡å‹å“åº” ---\")\n",
    "            print(responseText)\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- å®¢æˆ·ç«¯ä½¿ç”¨ç¤ºä¾‹ ---\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
