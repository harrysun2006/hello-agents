# version: "3.9"

services:
  # DeepSeek-R1-Distill-Qwen-7B > Qwen2.5-Coder-7B-Instruct > Gemma-2-9B-it
  # NOTE: 模型名称区分大小写, Qwen/Qwenxxx, not QWen/Qwenxxx !!!
  # 大多数用户：用 Qwen2.5-Coder-7B-Instruct，体验最好。
  # 做代码补全工具：用 Qwen2.5-Coder-7B。
  tgi-deepseek:
    image: ghcr.io/huggingface/text-generation-inference:latest
    container_name: tgi
    # restart: unless-stopped
    profiles: ["deepseek"]
    runtime: nvidia
    ports:
      - "8080:80"
    networks:
      tgi:
        ipv4_address: 172.19.0.2
    environment:
      # 如模型为 gated 或私有，需要 HF_TOKEN
      # HF_TOKEN: "hf_xxx"
      # HUGGINGFACE_HUB_CACHE: /data/cache
      NVIDIA_VISIBLE_DEVICES: "all"
      NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
    volumes:
      - /home/harry/.cache/huggingface:/data
    # GPU 支持（NVIDIA Compose 配置）
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    command:
      - "--model-id deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
      - "--dtype float16"
      - "--cuda-memory-fraction 0.9"
      - "--max-input-tokens 8192"
      - "--max-total-tokens 16384"

      # - "--max-batch-total-tokens 32768"
      # - "--max-batch-prefill-tokens 32768"
      # - "--max-batch-size 4"
      # - "--num-shard 1"
      # - "--quantize bitsandbytes"
      # - "--quantize bitsandbytes-nf4"

    # obsolete:
    #   --gpu-memory-utilization 0.9

  tgi-qwen25-7b:
    image: ghcr.io/huggingface/text-generation-inference:latest
    container_name: tgi
    # restart: unless-stopped
    profiles: ["qwen25-7b"]
    runtime: nvidia
    ports:
      - "8080:80"
    networks:
      tgi:
        ipv4_address: 172.19.0.2
    volumes:
      - /home/harry/.cache/huggingface:/data
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    command:
      - "--model-id Qwen/Qwen2.5-Coder-7B-Instruct" # for instruction following
      # - "--model-id Qwen/Qwen2.5-Coder-7B"        # for code completion
      - "--dtype float16"
      - "--cuda-memory-fraction 0.9"
      - "--max-input-tokens 8192"
      - "--max-total-tokens 16384"

  tgi-gemma2-9b:
    image: ghcr.io/huggingface/text-generation-inference:latest
    container_name: tgi
    # restart: unless-stopped
    profiles: ["gemma2-9b"]
    runtime: nvidia
    ports:
      - "8080:80"
    networks:
      tgi:
        ipv4_address: 172.19.0.2
    volumes:
      - /home/harry/.cache/huggingface:/data
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    command:
      - "--model-id google/Gemma-2-9B-it"
      - "--dtype float16"
      - "--cuda-memory-fraction 0.9"
      - "--max-input-tokens 8192"
      - "--max-total-tokens 16384"

  tgi-qwen25-14b:
    image: ghcr.io/huggingface/text-generation-inference:latest
    container_name: tgi
    # restart: unless-stopped
    profiles: ["qwen25-14b"]
    runtime: nvidia
    ports:
      - "8080:80"
    networks:
      tgi:
        ipv4_address: 172.19.0.2
    volumes:
      - /home/harry/.cache/huggingface:/data
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    command:
      - "--model-id Qwen/Qwen2.5-Coder-14B-Instruct"
      - "--quantize bitsandbytes"
      - "--dtype float16"
      - "--cuda-memory-fraction 0.9"
      - "--max-input-tokens 8192"
      - "--max-total-tokens 16384"

  tgi-qwen3-8b:
    image: ghcr.io/huggingface/text-generation-inference:latest
    container_name: tgi
    # restart: unless-stopped
    profiles: ["qwen3-8b"]
    runtime: nvidia
    ports:
      - "8080:80"
    networks:
      tgi:
        ipv4_address: 172.19.0.2
    volumes:
      - /home/harry/.cache/huggingface:/data
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    command:
      - "--model-id Qwen/Qwen3-8B"
      - "--dtype float16"
      - "--cuda-memory-fraction 0.9"
      - "--max-input-tokens 28000"
      - "--max-total-tokens 32000"

  tgi-qwen3-4b:
    image: ghcr.io/huggingface/text-generation-inference:latest
    container_name: tgi
    # restart: unless-stopped
    profiles: ["qwen3-4b"]
    runtime: nvidia
    ports:
      - "8080:80"
    networks:
      tgi:
        ipv4_address: 172.19.0.2
    volumes:
      - /home/harry/.cache/huggingface:/data
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    command:
      - "--model-id Qwen/Qwen3-4B"
      - "--dtype float16"
      - "--cuda-memory-fraction 0.9"
      - "--max-input-tokens 28000"
      - "--max-total-tokens 32000"

  tgi-qwen3-1d7b:
    image: ghcr.io/huggingface/text-generation-inference:latest
    container_name: tgi
    # restart: unless-stopped
    profiles: ["qwen3-1.7b"]
    runtime: nvidia
    ports:
      - "8080:80"
    networks:
      tgi:
        ipv4_address: 172.19.0.2
    volumes:
      - /home/harry/.cache/huggingface:/data
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    command:
      - "--model-id Qwen/Qwen3-1.7B"
      - "--dtype float16"
      - "--cuda-memory-fraction 0.9"
      - "--max-input-tokens 28000"
      - "--max-total-tokens 32000"

  tgi-qwen3-0d6b:
    image: ghcr.io/huggingface/text-generation-inference:latest
    container_name: tgi
    # restart: unless-stopped
    profiles: ["qwen3-0.6b"]
    runtime: nvidia
    ports:
      - "8080:80"
    networks:
      tgi:
        ipv4_address: 172.19.0.2
    volumes:
      - /home/harry/.cache/huggingface:/data
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    command:
      - "--model-id Qwen/Qwen3-0.6B"
      - "--dtype float16"
      - "--cuda-memory-fraction 0.9"
      - "--max-input-tokens 28000"
      - "--max-total-tokens 32000"

  mongo:
    image: mongo:7
    container_name: chatui-mongo
    # restart: unless-stopped
    ports:
      - "27017:27017"
    networks:
      tgi:
        ipv4_address: 172.19.0.3
    volumes:
      - chatui-mongo-data:/data/db

  chat-ui:
    image: ghcr.io/huggingface/chat-ui-db:latest
    container_name: chat-ui
    # restart: unless-stopped
    depends_on:
      - mongo
    ports:
      - "3000:3000"
    networks:
      tgi:
        ipv4_address: 172.19.0.5
    environment:
      MONGODB_URL: "mongodb://mongo:27017"
      # OPENAI_BASE_URL: "http://host.docker.internal:8000/v1"  # vLLM
      # OPENAI_BASE_URL: "http://host.docker.internal:8080/v1"  # TGI
      OPENAI_BASE_URL: "http://tgi:80/v1"  # TGI
      OPENAI_API_KEY: "dummy"
      HF_TOKEN: ${HF_TOKEN}
      PUBLIC_ORIGIN: ${PUBLIC_ORIGIN}
      ALLOW_INSECURE_COOKIES: "true"
      TEST_ENV: "test"
    volumes:
      - chatui-data:/data
    # extra_hosts:
    # this is not pointing to the host machine IP, but to the docker internal network
    #   - "host.docker.internal:host-gateway"

volumes:
  chatui-mongo-data:
  chatui-data:

networks:
  tgi:
    name: tgi
    # external: true
    driver: bridge
    ipam:
      config:
        - subnet: 172.19.0.0/16
          gateway: 172.19.0.1