import difflib, os, re, json, requests, time

from openai import OpenAI
from dotenv import load_dotenv
from typing import Optional, List, Dict, Any
from tasks import LC303, LC11, LC198, LC65, LC862, LC3225
from prompts_cn_v2 import SYSTEM_PROMPT, INITIAL_PROMPT_TEMPLATE, REFLECT_PROMPT_TEMPLATE, REFINE_PROMPT_TEMPLATE

load_dotenv(dotenv_path='.env', override=True)
print(f'LLM_BASE_URL={os.getenv("LLM_BASE_URL")}')

def extract_block(text: str, tag: str) -> str:
    """
    æå– <tag></tag> æ ‡ç­¾å†…å®¹ï¼Œç”¨äºè°ƒè¯•æŸ¥çœ‹æ¨¡å‹æ€è·¯
    """
    pattern = rf"<{tag}>(.*?)</{tag}>"
    match = re.search(pattern, text, re.DOTALL)
    return match.group(1).strip() if match else None

# --- æ¨¡å— 1: è®°å¿†æ¨¡å— ---
class Memory:
    """
    ä¸€ä¸ªç®€å•çš„çŸ­æœŸè®°å¿†æ¨¡å—ï¼Œç”¨äºå­˜å‚¨æ™ºèƒ½ä½“çš„è¡ŒåŠ¨ä¸åæ€è½¨è¿¹ã€‚
    """
    def __init__(self):
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨æ‰€æœ‰è®°å½•
        self.records: List[Dict[str, Any]] = []

    def add_record(self, record_type: str, content: str):
        """
        å‘è®°å¿†ä¸­æ·»åŠ ä¸€æ¡æ–°è®°å½•ã€‚

        å‚æ•°:
        - record_type (str): è®°å½•çš„ç±»å‹ ('execution' æˆ– 'reflection')ã€‚
        - content (str): è®°å½•çš„å…·ä½“å†…å®¹ (ä¾‹å¦‚ï¼Œç”Ÿæˆçš„ä»£ç æˆ–åæ€çš„åé¦ˆ)ã€‚
        """
        self.records.append({"type": record_type, "content": content})
        print(f"ğŸ“ è®°å¿†å·²æ›´æ–°ï¼Œæ–°å¢ä¸€æ¡ '{record_type}' è®°å½•ã€‚")

    def get_trajectory(self) -> str:
        """
        å°†æ‰€æœ‰è®°å¿†è®°å½•æ ¼å¼åŒ–ä¸ºä¸€ä¸ªè¿è´¯çš„å­—ç¬¦ä¸²æ–‡æœ¬ï¼Œç”¨äºæ„å»ºæç¤ºè¯ã€‚
        """
        trajectory = ""
        for record in self.records:
            if record['type'] == 'execution':
                trajectory += f"--- ä¸Šä¸€è½®å°è¯• (ä»£ç ) ---\n{record['content']}\n\n"
            elif record['type'] == 'reflection':
                trajectory += f"--- è¯„å®¡å‘˜åé¦ˆ ---\n{record['content']}\n\n"
        return trajectory.strip()

    def get_last_execution(self) -> str:
        """
        è·å–æœ€è¿‘ä¸€æ¬¡çš„æ‰§è¡Œç»“æœ (ä¾‹å¦‚ï¼Œæœ€æ–°ç”Ÿæˆçš„ä»£ç )ã€‚
        """
        for record in reversed(self.records):
            if record['type'] == 'execution':
                return record['content']
        return None

class Watchdog:
    def __init__(self, threshold=0.85, max_consecutive_loops=3):
        self.threshold = threshold
        self.max_consecutive_loops = max_consecutive_loops
        self.history_lines = []
        self.consecutive_loops = 0

    def is_looping(self, new_text_chunk):
        """
        æ£€æµ‹æ–°ç”Ÿæˆçš„æ–‡æœ¬æ˜¯å¦åœ¨é‡å¤ä¹‹å‰çš„åºŸè¯ã€‚
        """
        # ç®€å•çš„æŒ‰è¡Œåˆ†å‰²ï¼Œå®é™…ä½¿ç”¨ä¸­å¯èƒ½éœ€è¦ç´¯ç§¯ buffer
        lines = new_text_chunk.strip().split('\n')
        
        for line in lines:
            if not line.strip():
                continue
                
            # æ£€æŸ¥ä¸ä¸Šä¸€è¡Œæˆ–ä¸Šä¸Šè¡Œçš„ç›¸ä¼¼åº¦
            if self.history_lines:
                last_line = self.history_lines[-1]
                # ä½¿ç”¨ SequenceMatcher è®¡ç®—ç›¸ä¼¼åº¦
                similarity = difflib.SequenceMatcher(None, line, last_line).ratio()
                
                if similarity > self.threshold:
                    self.consecutive_loops += 1
                else:
                    self.consecutive_loops = 0 # é‡ç½®
            
            self.history_lines.append(line)
            
            # ä¿æŒå†å²è®°å½•ä¸è¦å¤ªé•¿
            if len(self.history_lines) > 20:
                self.history_lines.pop(0)
                
            if self.consecutive_loops >= self.max_consecutive_loops:
                return True
                
        return False

# --- æ¨¡å— 2: Reflection æ™ºèƒ½ä½“ ---
class ReflectionAgent:
    def __init__(self, max_iterations=3):
        self.baseUrl = os.getenv("LLM_BASE_URL", "http://localhost:8080/v1")
        self.apiKey = os.getenv("LLM_API_KEY", "dumy_api_key")
        self.model = os.getenv("LLM_MODEL_ID", "AUTO")
        self.timeout = int(os.getenv("LLM_TIMEOUT", 60))
        if self.model == "AUTO":
            self.model = self._figure_model()

        if not all([self.model, self.apiKey, self.baseUrl]):
            raise ValueError("æ¨¡å‹IDã€APIå¯†é’¥å’ŒæœåŠ¡åœ°å€å¿…é¡»è¢«æä¾›æˆ–åœ¨.envæ–‡ä»¶ä¸­å®šä¹‰ã€‚")

        print(f"ğŸ› ï¸ åˆå§‹åŒ– LLM å®¢æˆ·ç«¯ï¼Œæ¨¡å‹: {self.model}, åœ°å€: {self.baseUrl}")
        self.client = OpenAI(api_key=self.apiKey, base_url=self.baseUrl, timeout=self.timeout)
        self.memory = Memory()
        self.max_iterations = max_iterations

    def _figure_model(self) -> str:
        response = requests.get(f'{self.baseUrl}/models', headers={'Authorization': f'Bearer {self.apiKey}'})
        response.raise_for_status()
        models = response.json().get('data', [])
        # ç®€å•é€‰æ‹©ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼Œå®é™…ä½¿ç”¨ä¸­å¯ä»¥æ ¹æ®éœ€æ±‚é€‰æ‹©
        if not models:
            raise ValueError("æœªèƒ½è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ã€‚")
        return models[0]['id']

    def run(self, task: str):
        print(f"\n--- å¼€å§‹å¤„ç†ä»»åŠ¡ ---\nä»»åŠ¡: {task}")

        watchdog = Watchdog(threshold=0.9, max_consecutive_loops=5)
        # --- 1. åˆå§‹æ‰§è¡Œ ---
        print("\n--- æ­£åœ¨è¿›è¡Œåˆå§‹å°è¯• ---")
        initial_prompt = INITIAL_PROMPT_TEMPLATE.format(task=task)
        # initial_code = self._think(initial_prompt)
        # brainstorming è®©æ¨¡å‹è¿›è¡Œå‘æ•£å‹æ€è€ƒï¼Œtemperature è®¾é«˜ä¸€ç‚¹
        initial_code = self._think(initial_prompt, temperature=0.5)
        initial_code = extract_block(initial_code, "code")
        self.memory.add_record("execution", initial_code)

        # --- 2. è¿­ä»£å¾ªç¯ï¼šåæ€ä¸ä¼˜åŒ– ---
        for i in range(self.max_iterations):
            print(f"\n--- ç¬¬ {i+1}/{self.max_iterations} è½®è¿­ä»£ ---")

            # a. åæ€
            print("\n-> æ­£åœ¨è¿›è¡Œåæ€...")
            last_code = self.memory.get_last_execution()
            reflect_prompt = REFLECT_PROMPT_TEMPLATE.format(task=task, code=last_code)
            feedback = self._think(reflect_prompt, temperature=0.1)
            feedback = extract_block(feedback, "feedback")
            self.memory.add_record("reflection", feedback)

            if not feedback:
                print("âŒ æ— æ³•è§£æåé¦ˆï¼Œç»“æŸè¿­ä»£ã€‚")
                break

            # b. æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
            if "æ— éœ€æ”¹è¿›" in feedback or "done" in feedback.lower():
                print("\nâœ… åæ€è®¤ä¸ºä»£ç å·²æ— éœ€æ”¹è¿›ï¼Œä»»åŠ¡å®Œæˆã€‚")
                break

            # c. ä¼˜åŒ–
            print("\n-> æ­£åœ¨è¿›è¡Œä¼˜åŒ–...")
            refine_prompt = REFINE_PROMPT_TEMPLATE.format(
                task=task,
                last_code_attempt=last_code,
                feedback=feedback
            )
            refined_code = self._think(refine_prompt, temperature=0)
            refined_code = extract_block(refined_code, "code")
            self.memory.add_record("execution", refined_code)
        
        final_code = self.memory.get_last_execution()
        print(f"\n--- ä»»åŠ¡å®Œæˆ ---\næœ€ç»ˆç”Ÿæˆçš„ä»£ç :\n```python\n{final_code}\n```")
        return final_code

    def _think(self, prompt: str, temperature=0.1, max_tokens=65536, topK=30, topP=0.8) -> str:
        """ä¸€ä¸ªè¾…åŠ©æ–¹æ³•ï¼Œç”¨äºè°ƒç”¨LLMå¹¶è·å–å®Œæ•´çš„æµå¼å“åº”ã€‚"""
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ]
        print(f"ğŸ§  æ­£åœ¨è°ƒç”¨ {self.model} æ¨¡å‹...")
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                stream=True,
            )
            
            # å¤„ç†æµå¼å“åº”
            print("âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:")
            collected_content = []
            for chunk in response:
                content = chunk.choices[0].delta.content or ""
                print(content, end="", flush=True)
                collected_content.append(content)
            print()  # åœ¨æµå¼è¾“å‡ºç»“æŸåæ¢è¡Œ
            return "".join(collected_content)

        except Exception as e:
            print(f"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None

def main():
    # 1. åˆå§‹åŒ– Reflection æ™ºèƒ½ä½“ï¼Œè®¾ç½®æœ€å¤šè¿­ä»£5è½®
    agent = ReflectionAgent(max_iterations=5)

    # 2. å®šä¹‰ä»»åŠ¡å¹¶è¿è¡Œæ™ºèƒ½ä½“
    tasks = [LC303, LC11, LC198, LC65, LC862, LC3225]
    task = tasks[5]

    start = time.time()
    print(f'\n=== æç¤ºè¯ ===')
    print(f'\n=== SYSTEM_PROMPT ===\n{SYSTEM_PROMPT}')
    print(f'\n=== INITIAL_PROMPT_TEMPLATE ===\n{INITIAL_PROMPT_TEMPLATE}')
    print(f'\n=== REFLECT_PROMPT_TEMPLATE ===\n{REFLECT_PROMPT_TEMPLATE}')
    print(f'\n=== REFINE_PROMPT_TEMPLATE ===\n{REFINE_PROMPT_TEMPLATE}')
    print(f'\n=== ä»»åŠ¡å¼€å§‹æ—¶é—´: {time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(start))} ===')
    agent.run(task)
    end = time.time()
    print(f'\n=== ä»»åŠ¡ç»“æŸæ—¶é—´: {time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(end))} ===')
    print(f'\n=== ä»»åŠ¡æ€»è€—æ—¶: {end - start:.2f} ç§’ ===')

if __name__ == '__main__':
    main()
