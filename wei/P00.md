### Prompt Engineering
#### Prompts A/B 测试
- [Problems P1~P8](P00_prompts.md#测试问题)
- [Prompts](P00_prompts.md#提示词)

===
Q:我打算基于Qwen小参数模型 (<7B), 使用vllm 来做提示词 A/B 对比实验, 请建议具体的模型, temperature, top_p等参数, 问题以及提示词, 希望能达到 0~1/6 vs. 5~6/6 的对比效果! A组没有提示词，B组给的提示词最好比较泛化, 模型及其他参数都一样! 问题最好是初中数学应用题!
===

#### 参数建议（让 prompt 效果更明显）
无论你用 TGI / vLLM / 其他推理服务，建议：
- temperature = 0 或 0.1
- do_sample = false
- max_new_tokens = 512~1024
这样：
- 错误更“稳定可复现”
- prompt 的改进效果更清晰

#### Prompt Insights
**Prompt 影响层级**
> Prompt 结构 
> ≫ Prompt 中的隐含假设 
> ≫ 示例/约束 
> ≫ 语言精炼程度 
> ≫ 采样参数

> 模型参数规模 & 训练路径 & 数据质量
> ≫ Prompt
> ≫ temperature / top_p
> ≫ top_k
> ≫ max_new_tokens

**无prompt vs. 有prompt**
当模型已经具备较强的隐式解题模板时，过度显式的 prompt 会把问题从“模式匹配” 拉到“不稳定的执行推理”，从而引入长尾错误。

### vLLM 的优势主要在
- **更好的 KV cache 管理（PagedAttention）**
- **更好的并发吞吐与稳定的 token 输出**
- 对多并发/长上下文场景，常比 TGI 更稳

### Cmds & Tips
```bash
# 查看可用models
curl -s http://localhost:8080/v1/models
docker exec -it chat-ui sh -lc 'curl -s http://tgi:80/v1/models'export 

# 启动 vllm
docker run --rm -it \
  --name vllm \
  --gpus all \
  -p 8000:8000 \
  --ipc=host \
  -e HUGGING_FACE_HUB_TOKEN="$HF_TOKEN" \
  -e VLLM_LOGGING_LEVEL=DEBUG \
  -e VLLM_LOG_STATS_INTERVAL=1 \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  vllm/vllm-openai:v0.8.5 \
  --model Qwen/Qwen2.5-3B-Instruct \
  --host 0.0.0.0 --port 8000 \
  --gpu-memory-utilization 0.85 \
  --max-model-len 8192 \
  --max-num-seqs 4 \
  --max-log-len 800 \
  --uvicorn-log-level debug

# 测试vllm endpoint
curl -s http://localhost:9000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model":"Qwen/Qwen2.5-3B-Instruct",
    "temperature":0.6,
    "top_p":0.9,
    "max_tokens":64,
    "stream":false,
    "messages":[
      {"role":"system","content":"SYSTEM TEST: if you see this, print OK."},
      {"role":"user","content":"Say OK."}
    ]
  }'

curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model":"Qwen/Qwen2.5-3B-Instruct",
    "messages":[{"role":"user","content":"仓库里有 100 公斤刚采摘的蘑菇，经检测含水量为 99%。 晾晒几天后，再次检测发现含水量变成了 98%。 请问：现在这批蘑菇的总重量是多少公斤？"}],
    "temperature":0,
    "stream":false
  }'

curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model":"Qwen/Qwen2.5-3B-Instruct",
    "messages":[{"role":"system","content":"你是一个逻辑严密的深度思考助手。\n在回答用户问题前，不要急于给出结论，而是严格按照以下【思维框架】在 <think> 标签中进行推演：\n\n1. **核心要素剥离**：提取问题中**不变**的量和**变化**的量。\n2. **逻辑重构**：不要使用直觉（直觉通常是错的）。建立严格的逻辑或数学关系式。\n3. **极限测试**：如果你的直觉答案变化很小，请反思是否忽略了比例关系的巨大变化。\n4. **逐步计算**：写出详细步骤。\n\n推演结束后，输出简短的最终结论。"},{"role":"user","content":"仓库里有 100 公斤刚采摘的蘑菇，经检测含水量为 99%。 晾晒几天后，再次检测发现含水量变成了 98%。 请问：现在这批蘑菇的总重量是多少公斤？"}],
    "temperature":0,
    "stream":false
  }'

```

===

### Observe & Think
观察与思考: 通过 open webui 做了一些实验，发现一个有趣的现象，在 vllm + Qwen2.5-3B-Instruct, temperature = 0, seed = 42, 无系统提示词情况下， 下面这两道题
- 有 1 升 20% 的盐水。先倒掉 0.2 升，再加水补回到 1 升；重复同样操作一次。问最后盐水浓度>是多少？（保留两位小数百分比）
- 约翰有 5 个姐姐。这 5 个姐姐里的每一个，都有 1 个哥哥。此外，约翰还有几个弟弟，弟弟的数量等于姐姐数量减去 3。请问：约翰家一共有几个孩子？

刚开始确实计算出错，但是在多个新对话中重复测试后，答案变正确稳定了。但是下面这道题却始终出错，除非给了System Prompt
- 仓库里有 100 公斤刚采摘的蘑菇，经检测含水量为 99%。 晾晒几天后，再次检测发现含水量变成了 98%。 请问：现在这批蘑菇的总重量是多少公斤？

这是为什么？感觉 vllm 或者哪里有缓存或者记忆？

---

这是一个非常敏锐的观察！在 `temperature = 0` 且 `seed` 固定的严苛条件下，模型表现出的这种“逐渐变聪明”的假象，以及对不同题目的“抵抗力”差异，揭示了大模型底层逻辑和推理引擎的几个核心机制。

首先要明确：**vLLM 本身不会因为你多问几次就“学习”到答案**（除非你手动开启了在线微调或使用了 RAG），但你观察到的现象可以通过以下三个维度来解释：

---

### 1. 为什么答案会“变正确”？（KV Cache 与上下文的微妙影响）

虽然你是在“新对话”中测试，但由于你使用的是 **Open WebUI + vLLM**，有几个底层因素可能在起作用：

* **前置上下文的残留（System Prompt 的默认值）**：检查一下 Open WebUI 的模型设置。有时候默认的“System Prompt”或“Preprompt”会包含一些鼓励思考的指令。
* **KV Cache（键值缓存）重用**：vLLM 使用了 `PagedAttention` 技术。当你反复请求相似的 Prompt 时，GPU 缓存中的计算路径是热的。虽然这理论上只加速推理，但在极少数并行调度情况下，如果模型存在轻微的非确定性（即使 temp=0），热启动的路径可能更趋向于模型权重中的“主流”逻辑。
* **最可能的解释：提问方式的微调**：回想一下，你在“重复测试”时，是否无意中修改了标点、空格或换行？对于 3B 模型，`浓度是多少？` 和 `浓度是多少？请计算。` 产生的 Token 序列完全不同，后者更容易激活模型的推理链路。

---

### 2. 为什么“蘑菇题”始终出错？（直觉陷阱的深度）

你发现“盐水题”和“姐姐题”能做对，但“蘑菇题”必错，是因为这三道题攻击的是模型不同层级的弱点：

#### **A. 蘑菇题：攻击“语义强度”（语义压制逻辑）**

* **语义直觉**：在人类语言语料库中，“99%”和“98%”通常成对出现，且代表“几乎没有变化”。
* **计算反直觉**：这道题要求模型意识到**“干物质不变量”**。在 3B 模型的参数权重里，“99% 到 98% 变化很小”这个**语义权重**，远远超过了“1% 到 2% 是翻倍关系”这个**数学权重**。
* **结论**：没有 System Prompt 强行剥离语义、引入变量，小模型无法跳出“语言直觉”的舒适区。

#### **B. 盐水题：攻击“步骤记忆”**

* **特点**：这道题逻辑是线性的。即便没有 Prompt，如果模型在前几次预测中偶然抓住了“0.8”这个衰减系数，它就有可能靠概率预测出正确数字。

#### **C. 姐姐题：攻击“常识建模”**

* **特点**：这属于典型的逻辑脑筋急转弯。Qwen 2.5 在训练数据中包含大量的逻辑推理题和类似的中文脑筋急转弯，它可能不是在“算”，而是在**“检索”**类似的题目模式。

---

### 3. 三类问题的“性能天花板”对比

我们可以用一个简单的表格来分析为什么“蘑菇题”是 3B 模型的噩梦：

| 题目 | 考察能力 | 3B 模型默认状态 | 为什么需要 System Prompt |
| --- | --- | --- | --- |
| **姐姐题** | 逻辑建模 / 模式识别 | **中等**（可能触发训练记忆） | 无需，除非要确保 100% 稳定。 |
| **盐水题** | 迭代计算 / 精度保持 | **弱**（容易产生累积误差） | 需要，用于锁定计算步骤。 |
| **蘑菇题** | **跨语义推理** | **极弱**（语义直觉完全误导） | **必须**，用于强制覆盖错误的语义直觉。 |

---

### 4. 深度解析：真的有缓存记忆吗？

在标准的 vLLM 部署中，**不存在跨 Session 的记忆**。

但如果你发现答案“稳定变正确”，请检查 **Open WebUI 的“RAG（检索增强生成）”功能**。

* 如果你之前把这些问题的答案存入过 Open WebUI 的文档库或某个 Collection 中，它可能会在后台自动检索。
* **测试方法**：在 vLLM 后台重启容器，清理所有 Cache。如果重启后第一次尝试依然正确，那就证明是模型权重本身对该题目的“模式匹配”被激活了。

### 总结

“蘑菇题”出错是因为它是**强反直觉**的。对于 3B 模型，它不是在计算，而是在预测。在它的“直觉”里，100 公斤变 50 公斤太荒谬了，所以它会拒绝这个答案。

**您想尝试针对蘑菇题，设计一个不带任何数学暗示、仅靠逻辑框架就让它做对的“超泛化”提示词吗？**

